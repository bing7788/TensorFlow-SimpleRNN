{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RNN 直观理解\n",
    "+ 一个非常棒的RNN入门[Anyone Can learn To Code LSTM-RNN in Python(Part 1: RNN)](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n",
    "+ 基于此文章，本文给出我自己的一些愚见\n",
    "+ 基于此文章，给出其中代码的TensorFlow的实现版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN的结构\n",
    "如果从网上搜索关于RNN的结构图，大概可以下面的结构图\n",
    "![rnn_arch](rnn_arch.png)\n",
    "第一次看到这样的图，我是懵逼的，这货怎么有两种形态? 先说结论：\n",
    "+ 左侧是RNN的原始结构， 右侧是RNN在时间上展开的结果\n",
    "+ RNN的结构，本质上和全连接网络相同\n",
    "\n",
    "为什么可以根据时间维度展开，这主要是因为RNN的的输入是具有时间序列的。这一点是和全连接网络最大的不同，**输入决定了RNN的结构**\n",
    "假设RNN的输入是一句话，这句话中有多个单词，那么RNN需要forward多次，如下图\n",
    "![rnn_run](rnn_run.gif)\n",
    "+ 橙色部分是上一个时刻的隐层的值，可以直观的理解为“记忆”\n",
    "+ 当前时刻的输出与当前时刻的输入还有记忆有关。\n",
    "+ RNN对一个样本需要做多次forward，这一点与全连接网络不一样，全连接网络对一个样本只做一次forward。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 就将RNN看成是全连接网络吧\n",
    "\n",
    "将RNN看成是全连接网络就能很好的理解它。\n",
    "+ 将上面gif图中的隐层中的每一个神经元看成是LSTM单元，就得到了基于LSTM的RNN\n",
    "+ RNN的输入、输出都和全连接网络一模一样\n",
    "+ RNN只是一个需要做好多次forward的全连接网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个RNN的简单例子\n",
    "+ 基于TensorFlow，搭建一个RNN，教会神经网络进行二进制加法。参考[Anyone Can learn To Code LSTM-RNN in Python(Part 1: RNN)](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 一个字典，隐射一个数字到其二进制的表示\n",
    "# 例如 int2binary[3] = [0,0,0,0,0,0,1,1]\n",
    "int2binary = {}\n",
    "\n",
    "# 最多8位二进制\n",
    "binary_dim = 8\n",
    "\n",
    "# 在8位情况下，最大数为2^8 = 256\n",
    "largest_number = pow(2,binary_dim)\n",
    "\n",
    "# 将[0,256)所有数表示成二进制\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "\n",
    "# 建立字典\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "def binary_generation(numbers, reverse = False):\n",
    "    '''\n",
    "    返回numbers中所有数的二进制表达，\n",
    "    例如 numbers = [3, 2, 1]\n",
    "    返回 ：[[0,0,0,0,0,0,1,1],\n",
    "            [0,0,0,0,0,0,1,0],\n",
    "            [0,0,0,0,0,0,0,1]'\n",
    "            \n",
    "    如果 reverse = True, 二进制表达式前后颠倒，\n",
    "    这么做是为训练方便，因为训练的输入顺序是从低位开始的\n",
    "    \n",
    "    numbers : 一组数字\n",
    "    reverse : 是否将其二进制表示进行前后翻转\n",
    "    '''\n",
    "    binary_x = np.array([ int2binary[num] for num in numbers], dtype=np.uint8)\n",
    "    \n",
    "    if reverse:\n",
    "        binary_x = np.fliplr(binary_x)\n",
    "    \n",
    "    return binary_x\n",
    "\n",
    "def batch_generation(batch_size, largest_number):\n",
    "    '''\n",
    "    生成batch_size大小的数据，用于训练或者验证\n",
    "    \n",
    "    batch_x 大小为[batch_size, biniary_dim, 2]\n",
    "    batch_y 大小为[batch_size, biniray_dim]\n",
    "    '''\n",
    "\n",
    "    # 随机生成batch_size个数\n",
    "    n1 = np.random.randint(0, largest_number//2, batch_size)\n",
    "    n2 = np.random.randint(0, largest_number//2, batch_size)\n",
    "    # 计算加法结果\n",
    "    add = n1 + n2\n",
    "    \n",
    "    # int to binary\n",
    "    binary_n1 = binary_generation(n1, True)\n",
    "    binary_n2 = binary_generation(n2, True)\n",
    "    batch_y = binary_generation(add, True)\n",
    "    \n",
    "    # 堆叠，因为网络的输入是2个二进制\n",
    "    batch_x = np.dstack((binary_n1, binary_n2))\n",
    "    \n",
    "    return batch_x, batch_y, n1, n2, add\n",
    "\n",
    "def binary2int(binary_array):\n",
    "    '''\n",
    "    将一个二进制数组转为整数\n",
    "    '''\n",
    "    out = 0\n",
    "    for index, x in enumerate(reversed(binary_array)):\n",
    "        out += x*pow(2, index)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# LSTM的个数，就是隐层中神经元的数量\n",
    "lstm_size = 20\n",
    "# 隐层的层数\n",
    "lstm_layers =2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入，[None, binary_dim, 2], \n",
    "# None表示batch_size, binary_dim表示输入序列的长度，2表示每个时刻有两个输入\n",
    "x = tf.placeholder(tf.float32, [None, binary_dim, 2], name='input_x')\n",
    "\n",
    "# 输出\n",
    "y_ = tf.placeholder(tf.float32, [None, binary_dim], name='input_y')\n",
    "# dropout 参数\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建LSTM层（看成隐层）\n",
    "# 有lstm_size个单元\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# dropout\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "# 一层不够，就多来几层\n",
    "def lstm_cell():\n",
    "  return tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([ lstm_cell() for _ in range(lstm_layers)])\n",
    "\n",
    "# 初始状态，可以理解为初始记忆\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# 进行forward，得到隐层的输出\n",
    "# outputs 大小为[batch_size, lstm_size*binary_dim]\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=initial_state)\n",
    "\n",
    "# 建立输出层\n",
    "weights = tf.Variable(tf.truncated_normal([lstm_size, 1], stddev=0.01))\n",
    "bias = tf.zeros([1])\n",
    "\n",
    "# [batch_size, lstm_size*binary_dim] ==> [batch_size*binary_dim, lstm_size]\n",
    "outputs = tf.reshape(outputs, [-1, lstm_size])\n",
    "# 得到输出, logits大小为[batch_size*binary_dim, 1]\n",
    "logits = tf.sigmoid(tf.matmul(outputs, weights))\n",
    "# [batch_size*binary_dim, 1] ==> [batch_size, binary_dim]\n",
    "predictions = tf.reshape(logits, [-1, binary_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数和优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.losses.mean_squared_error(y_, predictions)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:1000, Loss:0.012912601232528687\n",
      "Iter:2000, Loss:0.000789149955380708\n",
      "[0 1 0 1 0 0 1 0]:82\n",
      "[0 0 1 1 1 1 0 0]:60\n",
      "[1 0 0 0 1 1 1 0]:142\n",
      "\n",
      "[0 1 1 0 0 0 1 0]:98\n",
      "[0 0 0 1 0 0 1 1]:19\n",
      "[0 1 1 1 0 1 0 1]:117\n",
      "\n",
      "[0 1 1 0 0 1 0 0]:100\n",
      "[0 1 1 0 0 1 0 1]:101\n",
      "[1 1 0 0 1 0 0 1]:201\n",
      "\n",
      "[0 0 0 1 0 0 1 0]:18\n",
      "[0 1 1 1 0 1 1 1]:119\n",
      "[1 0 0 0 1 0 0 1]:137\n",
      "\n",
      "[0 1 1 0 0 0 1 0]:98\n",
      "[0 0 1 1 1 0 0 1]:57\n",
      "[1 0 0 1 1 0 1 1]:155\n",
      "\n",
      "[0 1 1 1 1 1 0 0]:124\n",
      "[0 1 1 0 0 0 1 0]:98\n",
      "[1 1 0 1 1 1 1 0]:222\n",
      "\n",
      "[0 0 0 1 1 0 1 0]:26\n",
      "[0 0 0 0 0 1 0 1]:5\n",
      "[0 0 0 1 1 1 1 1]:31\n",
      "\n",
      "[0 0 0 1 0 0 1 1]:19\n",
      "[0 0 1 0 1 0 0 1]:41\n",
      "[0 0 1 1 1 1 0 0]:60\n",
      "\n",
      "[0 1 0 1 1 0 1 1]:91\n",
      "[0 1 0 0 0 0 0 0]:64\n",
      "[1 0 0 1 1 0 1 1]:155\n",
      "\n",
      "[0 1 0 1 0 1 0 0]:84\n",
      "[0 0 0 0 1 1 0 1]:13\n",
      "[0 1 1 0 0 0 0 1]:97\n",
      "\n",
      "[0 1 0 0 1 0 1 1]:75\n",
      "[0 0 1 0 0 0 0 1]:33\n",
      "[0 1 1 0 1 1 0 0]:108\n",
      "\n",
      "[0 0 1 1 0 1 0 0]:52\n",
      "[0 1 1 1 0 0 1 0]:114\n",
      "[1 0 1 0 0 1 1 0]:166\n",
      "\n",
      "[0 1 1 1 0 0 0 0]:112\n",
      "[0 0 1 0 1 1 0 1]:45\n",
      "[1 0 0 1 1 1 0 1]:157\n",
      "\n",
      "[0 0 1 1 0 1 0 1]:53\n",
      "[0 1 0 0 0 0 0 1]:65\n",
      "[0 1 1 1 0 1 1 0]:118\n",
      "\n",
      "[0 0 0 0 1 0 1 1]:11\n",
      "[0 1 1 0 0 1 0 1]:101\n",
      "[0 1 1 1 0 0 0 0]:112\n",
      "\n",
      "[0 0 0 0 1 1 0 0]:12\n",
      "[0 0 1 0 0 1 0 0]:36\n",
      "[0 0 1 1 0 0 0 0]:48\n",
      "\n",
      "[0 0 0 0 1 0 0 1]:9\n",
      "[0 1 1 0 0 0 0 1]:97\n",
      "[0 1 1 0 1 0 1 0]:106\n",
      "\n",
      "[0 0 0 1 1 1 1 0]:30\n",
      "[0 1 0 0 0 0 0 1]:65\n",
      "[0 1 0 1 1 1 1 1]:95\n",
      "\n",
      "[0 0 1 1 1 1 0 0]:60\n",
      "[0 1 1 0 0 1 0 0]:100\n",
      "[1 0 1 0 0 0 0 0]:160\n",
      "\n",
      "[0 0 0 0 1 0 1 0]:10\n",
      "[0 1 0 1 1 0 1 0]:90\n",
      "[0 1 1 0 0 1 0 0]:100\n",
      "\n",
      "[0 1 1 1 0 1 1 1]:119\n",
      "[0 0 0 0 1 1 0 1]:13\n",
      "[1 0 0 0 0 1 0 0]:132\n",
      "\n",
      "[0 0 0 0 1 1 1 0]:14\n",
      "[0 0 1 1 1 0 0 1]:57\n",
      "[0 1 0 0 0 1 1 1]:71\n",
      "\n",
      "[0 0 0 1 0 0 1 1]:19\n",
      "[0 1 0 0 0 0 0 1]:65\n",
      "[0 1 0 1 0 1 0 0]:84\n",
      "\n",
      "[0 1 1 0 1 1 0 0]:108\n",
      "[0 0 1 0 0 0 0 1]:33\n",
      "[1 0 0 0 1 1 0 1]:141\n",
      "\n",
      "[0 0 1 1 1 1 1 1]:63\n",
      "[0 0 1 0 0 0 0 1]:33\n",
      "[0 1 1 0 0 0 0 0]:96\n",
      "\n",
      "[0 0 0 1 0 1 1 1]:23\n",
      "[0 0 0 1 1 1 0 1]:29\n",
      "[0 0 1 1 0 1 0 0]:52\n",
      "\n",
      "[0 0 1 1 1 1 0 0]:60\n",
      "[0 1 0 0 1 0 0 1]:73\n",
      "[1 0 0 0 0 1 0 1]:133\n",
      "\n",
      "[0 1 0 0 1 1 0 0]:76\n",
      "[0 1 1 1 0 1 1 1]:119\n",
      "[1 1 0 0 0 0 1 1]:195\n",
      "\n",
      "[0 1 1 0 1 1 1 1]:111\n",
      "[0 0 0 1 1 0 0 1]:25\n",
      "[1 0 0 0 1 0 0 0]:136\n",
      "\n",
      "[0 0 1 1 0 0 0 1]:49\n",
      "[0 1 1 0 1 0 0 0]:104\n",
      "[1 0 0 1 1 0 0 1]:153\n",
      "\n",
      "[0 1 1 1 1 1 0 0]:124\n",
      "[0 1 1 0 1 0 1 1]:107\n",
      "[1 1 1 0 0 1 1 1]:231\n",
      "\n",
      "[0 0 1 1 1 0 1 1]:59\n",
      "[0 0 0 1 1 0 1 1]:27\n",
      "[0 1 0 1 0 1 1 0]:86\n",
      "\n",
      "[0 1 1 0 0 0 0 0]:96\n",
      "[0 0 0 1 1 1 0 1]:29\n",
      "[0 1 1 1 1 1 0 1]:125\n",
      "\n",
      "[0 0 1 1 1 0 1 0]:58\n",
      "[0 1 0 0 0 1 0 0]:68\n",
      "[0 1 1 1 1 1 1 0]:126\n",
      "\n",
      "[0 0 1 0 1 0 1 1]:43\n",
      "[0 1 1 1 0 1 1 1]:119\n",
      "[1 0 1 0 0 0 1 0]:162\n",
      "\n",
      "[0 0 0 0 1 1 1 1]:15\n",
      "[0 1 0 0 1 0 0 1]:73\n",
      "[0 1 0 1 1 0 0 0]:88\n",
      "\n",
      "[0 0 1 1 1 1 1 0]:62\n",
      "[0 0 0 0 1 0 0 0]:8\n",
      "[0 1 0 0 0 1 1 0]:70\n",
      "\n",
      "[0 0 0 0 0 0 1 1]:3\n",
      "[0 0 1 0 0 1 1 0]:38\n",
      "[0 0 1 0 1 0 0 1]:41\n",
      "\n",
      "[0 0 1 1 1 1 1 0]:62\n",
      "[0 1 1 0 0 1 1 1]:103\n",
      "[1 0 1 0 0 1 0 1]:165\n",
      "\n",
      "[0 1 1 1 1 0 0 0]:120\n",
      "[0 0 1 0 1 1 1 0]:46\n",
      "[1 0 1 0 0 1 1 0]:166\n",
      "\n",
      "[0 1 0 0 1 0 0 1]:73\n",
      "[0 1 1 1 0 0 1 1]:115\n",
      "[1 0 1 1 1 1 0 0]:188\n",
      "\n",
      "[0 1 0 1 0 0 1 0]:82\n",
      "[0 0 1 1 0 1 0 0]:52\n",
      "[1 0 0 0 0 1 1 0]:134\n",
      "\n",
      "[0 1 1 0 0 1 0 0]:100\n",
      "[0 0 1 0 0 1 1 1]:39\n",
      "[1 0 0 0 1 0 1 1]:139\n",
      "\n",
      "[0 0 1 1 0 1 1 0]:54\n",
      "[0 1 1 0 1 0 0 1]:105\n",
      "[1 0 0 1 1 1 1 1]:159\n",
      "\n",
      "[0 0 0 0 1 1 0 1]:13\n",
      "[0 0 1 1 0 0 0 0]:48\n",
      "[0 0 1 1 1 1 0 1]:61\n",
      "\n",
      "[0 0 1 0 0 1 0 0]:36\n",
      "[0 0 1 1 0 0 0 1]:49\n",
      "[0 1 0 1 0 1 0 1]:85\n",
      "\n",
      "[0 1 0 1 1 1 1 0]:94\n",
      "[0 0 0 0 0 0 0 1]:1\n",
      "[0 1 0 1 1 1 1 1]:95\n",
      "\n",
      "[0 0 0 0 0 0 1 0]:2\n",
      "[0 0 0 0 0 1 0 0]:4\n",
      "[0 0 0 0 0 1 1 0]:6\n",
      "\n",
      "[0 0 1 1 1 1 0 0]:60\n",
      "[0 1 0 0 0 1 1 0]:70\n",
      "[1 0 0 0 0 0 1 0]:130\n",
      "\n",
      "[0 1 0 1 1 0 0 0]:88\n",
      "[0 1 0 0 0 1 0 1]:69\n",
      "[1 0 0 1 1 1 0 1]:157\n",
      "\n",
      "[0 1 0 0 0 0 1 1]:67\n",
      "[0 1 1 1 0 1 1 1]:119\n",
      "[1 0 1 1 1 0 1 0]:186\n",
      "\n",
      "[0 0 1 1 1 0 1 1]:59\n",
      "[0 1 0 0 0 0 1 1]:67\n",
      "[0 1 1 1 1 1 1 0]:126\n",
      "\n",
      "[0 0 1 1 1 1 1 1]:63\n",
      "[0 0 0 1 0 1 1 0]:22\n",
      "[0 1 0 1 0 1 0 1]:85\n",
      "\n",
      "[0 1 1 0 1 1 1 1]:111\n",
      "[0 0 1 1 1 1 0 0]:60\n",
      "[1 0 1 0 1 0 1 1]:171\n",
      "\n",
      "[0 1 1 0 1 1 1 0]:110\n",
      "[0 0 1 1 0 1 1 0]:54\n",
      "[1 0 1 0 0 1 0 0]:164\n",
      "\n",
      "[0 1 0 1 0 1 0 0]:84\n",
      "[0 0 0 1 1 0 1 1]:27\n",
      "[0 1 1 0 1 1 1 1]:111\n",
      "\n",
      "[0 0 1 0 0 1 0 1]:37\n",
      "[0 0 0 0 1 1 1 0]:14\n",
      "[0 0 1 1 0 0 1 1]:51\n",
      "\n",
      "[0 0 1 0 0 0 0 0]:32\n",
      "[0 0 1 0 0 0 1 0]:34\n",
      "[0 1 0 0 0 0 1 0]:66\n",
      "\n",
      "[0 0 1 0 1 0 1 1]:43\n",
      "[0 0 1 1 1 1 1 0]:62\n",
      "[0 1 1 0 1 0 0 1]:105\n",
      "\n",
      "[0 1 1 0 1 0 1 1]:107\n",
      "[0 1 1 0 0 0 0 1]:97\n",
      "[1 1 0 0 1 1 0 0]:204\n",
      "\n",
      "[0 1 0 0 1 0 0 0]:72\n",
      "[0 1 1 1 1 1 1 0]:126\n",
      "[1 1 0 0 0 1 1 0]:198\n",
      "\n",
      "[0 0 1 0 0 1 1 0]:38\n",
      "[0 1 0 0 1 1 0 1]:77\n",
      "[0 1 1 1 0 0 1 1]:115\n",
      "\n",
      "[0 1 0 1 1 1 1 0]:94\n",
      "[0 0 1 1 1 1 1 1]:63\n",
      "[1 0 0 1 1 1 0 1]:157\n",
      "\n",
      "[0 0 0 0 0 0 1 0]:2\n",
      "[0 1 0 1 0 0 0 0]:80\n",
      "[0 1 0 1 0 0 1 0]:82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps = 2000\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    iteration = 1\n",
    "    for i in range(steps):\n",
    "        # 获取训练数据\n",
    "        input_x, input_y,_,_,_ = batch_generation(batch_size, largest_number)\n",
    "        _, loss = sess.run([optimizer, cost], feed_dict={x:input_x, y_:input_y, keep_prob:0.5})\n",
    "        \n",
    "        if iteration % 1000 == 0:\n",
    "            print('Iter:{}, Loss:{}'.format(iteration, loss))    \n",
    "        iteration += 1\n",
    "    \n",
    "    # 训练结束，进行测试\n",
    "    val_x, val_y, n1, n2, add = batch_generation(batch_size, largest_number)\n",
    "    result = sess.run(predictions, feed_dict={x:val_x, y_:val_y, keep_prob:1.0})\n",
    "            \n",
    "    # 左右翻转二进制数组。因为输出的结果是低位在前，而正常的表达是高位在前，因此进行翻转\n",
    "    result = np.fliplr(np.round(result))\n",
    "    result = result.astype(np.int32)\n",
    "            \n",
    "    for  b_x, b_p, a, b, add in zip(np.fliplr(val_x), result, n1, n2, add):\n",
    "        print('{}:{}'.format(b_x[:,0], a))\n",
    "        print('{}:{}'.format(b_x[:,1], b))\n",
    "        print('{}:{}\\n'.format(b_p, binary2int(b_p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "本文向大家介绍了RNN的结构，以及从全连接网络的角度出去，去理解RNN，得到结论有：\n",
    "+ RNN的结构与全连接网络基本一致\n",
    "+ RNN具有时间展开的特点，这是由其输入决定的\n",
    "+ 全连接网络对一个样本做一次forward，RNN对一个样本做多次forward\n",
    "\n",
    "同时，基于TensorFlow给出一个简单的RNN网络，从实验结果看，该网络经过训练，以及能够正确进行二进制加法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
